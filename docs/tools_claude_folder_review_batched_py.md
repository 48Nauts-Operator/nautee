<!-- Auto-generated by Claude on 2025-06-23 04:34 -->

# Claude Folder Review (Batched) Documentation

## Overview

This Python script automates code review by analyzing source code files from a specified folder using Anthropic's Claude AI in batches. It intelligently processes large codebases by splitting them into manageable chunks and generates detailed markdown reviews for each batch.

## Purpose

- **Automated Code Review**: Leverages Claude AI to perform comprehensive code analysis
- **Batch Processing**: Handles large codebases by splitting files into ~10k token chunks
- **Documentation Generation**: Creates structured markdown reports for each batch
- **Scalable Analysis**: Processes multiple file types across entire directory structures

## Key Features

- Recursive folder scanning
- Token-based batching to stay within API limits
- Automatic exclusion of test files
- Support for multiple programming languages
- Timestamped markdown output

## Configuration

### Environment Variables

The script requires the following environment variables (typically in a `.env` file):

```bash
ANTHROPIC_API_KEY=your_api_key_here
ANTHROPIC_MODEL=claude-sonnet-4-20250514  # Optional, defaults to this model
```

### Supported File Types

```python
valid_exts = (".py", ".js", ".ts", ".tsx", ".jsx", ".html", ".css", ".json", ".go", ".java", ".yaml", ".yml")
```

## Important Functions

### `is_excluded(filename: str) -> bool`

Determines whether a file should be excluded from analysis.

**Parameters:**
- `filename` (str): The file path to check

**Returns:**
- `bool`: True if the file should be excluded (test files), False otherwise

**Logic:**
- Excludes files containing "test" in the name
- Excludes files starting with "test_"
- Excludes files in test directories

### `estimate_tokens(text: str) -> int`

Provides a rough estimate of token count for API usage planning.

**Parameters:**
- `text` (str): Source code content

**Returns:**
- `int`: Estimated token count (uses 4:1 character-to-token ratio)

**Note:** This is a simplified estimation. For production use, consider using tiktoken or similar libraries for more accurate token counting.

## Main Workflow

### 1. Setup Phase
```python
# Load environment variables
load_dotenv()
client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
```

### 2. File Discovery
- Recursively scans the target folder
- Filters files by supported extensions
- Excludes test files automatically

### 3. Token Batching
- Groups files into batches of ~10,000 tokens
- Prevents API limit exceeded errors
- Optimizes processing efficiency

### 4. Claude Analysis
- Sends each batch to Claude for review
- Focuses on:
  - Code organization
  - Bug patterns  
  - Architecture weaknesses
  - Modularization suggestions

### 5. Output Generation
- Creates individual markdown files for each batch
- Includes timestamps and structured formatting
- Saves to `docs/folder_review/` directory

## Usage

### Basic Usage
```bash
python claude_folder_review.py
```
*Defaults to reviewing `../137docs` folder*

### Custom Folder
```bash
python claude_folder_review.py /path/to/your/codebase
```

### Output Structure
```
docs/folder_review/
├── folder_review_batch_01.md
├── folder_review_batch_02.md
└── folder_review_batch_03.md
```

## Configuration Notes

### Token Limits
- **Current Limit**: 10,000 tokens per batch
- **Adjustable**: Modify `MAX_TOKENS` variable as needed
- **API Response**: Limited to 2,000 tokens output

### Model Selection
- **Default**: `claude-sonnet-4-20250514`
- **Customizable**: Set `ANTHROPIC_MODEL` environment variable
- **Recommendation**: Use latest Claude Sonnet for best code analysis

## Suggestions & Improvements

### Potential Enhancements

1. **Better Token Estimation**
   ```python
   # Consider using tiktoken for accurate counting
   import tiktoken
   
   def accurate_token_count(text: str) -> int:
       enc = tiktoken.get_encoding("cl100k_base")
       return len(enc.encode(text))
   ```

2. **Progress Tracking**
   - Add progress bars using libraries like `tqdm`
   - Include ETA calculations for large codebases

3. **Error Recovery**
   - Implement retry logic for API failures
   - Add checkpoint/resume functionality

4. **Output Formats**
   - Support JSON output for programmatic processing
   - Generate summary reports across all batches

5. **Configuration File**
   ```yaml
   # config.yaml
   file_extensions:
     - .py
     - .js
     - .ts
   exclude_patterns:
     - "**/test/**"
     - "**/*_test.*"
   max_tokens_per_batch: 10000
   ```

### Best Practices

- **API Key Security**: Never commit API keys to version control
- **Rate Limiting**: Consider adding delays between API calls for large batches
- **File Size Limits**: Very large files might need individual splitting
- **Review Output**: Always manually review generated analyses for accuracy

## Dependencies

```python
pip install anthropic python-dotenv
```

## Error Handling

The script includes basic error handling for:
- Missing or unreadable files
- Claude API failures
- Invalid file paths

For production use, consider implementing more robust error handling and logging mechanisms.