<!-- Auto-generated by Claude on 2025-05-31 21:34 -->

# Claude Review Tool Documentation

## Overview

The Claude Review Tool is a Python script that leverages Anthropic's Claude AI to perform automated code reviews. It operates in two modes: reviewing specific source files or analyzing Git diffs. The tool generates structured feedback in Markdown format, making it easy to integrate into development workflows.

## Purpose

- **Automated Code Review**: Get AI-powered feedback on code quality, bugs, and improvements
- **Git Integration**: Review changes in Git diffs automatically
- **Documentation Generation**: Output structured Markdown reviews for easy sharing and archiving
- **Development Workflow Enhancement**: Integrate AI reviews into CI/CD pipelines or manual review processes

## Requirements

### Dependencies
- `anthropic` - Anthropic's Claude API client
- `python-dotenv` - Environment variable management
- `subprocess` - Git command execution
- Standard library modules: `os`, `sys`, `datetime`

### Environment Variables
Create a `.env` file with the following variables:
```bash
ANTHROPIC_API_KEY=your_claude_api_key_here
ANTHROPIC_MODEL=claude-sonnet-4-20250514  # Optional, defaults to this model
```

## Usage

### Mode 1: File Review
Review specific source files by passing them as command-line arguments:
```bash
python claude_review.py file1.py file2.py module/file3.py
```

### Mode 2: Git Diff Review
Run without arguments to review Git changes:
```bash
python claude_review.py
```

## Key Functions

### `load_code(file_paths)`
Combines multiple source files into a single Markdown-formatted string for review.

**Parameters:**
- `file_paths` (list): List of file paths to process

**Returns:**
- `str`: Markdown-formatted string containing all file contents

**Features:**
- Skips missing files with warning messages
- Formats each file with proper Markdown code blocks
- Assumes Python syntax highlighting (hardcoded)

### `get_git_diff()`
Retrieves Git diff output with fallback strategies for different repository states.

**Returns:**
- `str`: Git diff content or empty string if unavailable

**Fallback Strategy:**
1. First attempts: `git diff origin/main...HEAD`
2. If failed, tries: `git diff HEAD^`
3. Returns empty string if both fail

### `main()`
The primary orchestrator function that handles:
- Environment setup and API client initialization
- Mode detection (file review vs. git diff)
- Claude API communication
- Output file generation

## Output

The tool generates a Markdown file at `docs/claude_review.md` containing:
- Timestamp of the review
- Structured feedback from Claude
- Analysis focusing on:
  - Bugs or logic issues
  - Code clarity and readability
  - Suggested improvements
  - Style consistency

## Configuration

### Model Selection
The default model is `claude-sonnet-4-20250514`, but can be customized via the `ANTHROPIC_MODEL` environment variable.

### Token Limit
Currently set to 1500 tokens maximum for responses. Adjust the `max_tokens` parameter in the API call if needed.

## Error Handling

- **Missing Files**: Warns and skips non-existent files
- **Git Errors**: Graceful fallback when Git commands fail
- **API Errors**: Catches and displays Claude API exceptions
- **No Diff Available**: Creates a stub review file when no changes are detected

## Notes and Suggestions

### Improvements Needed
- **Language Detection**: Currently hardcodes Python syntax highlighting - should detect file extensions
- **Configurable Output**: Output path is hardcoded to `docs/claude_review.md`
- **Token Management**: Fixed token limit may truncate large reviews

### Best Practices
- Ensure your `ANTHROPIC_API_KEY` is kept secure and not committed to version control
- The `docs/` directory is created automatically if it doesn't exist
- Consider running this tool as part of your pre-commit hooks or CI/CD pipeline

### Potential Enhancements
- Add support for multiple programming languages
- Implement configurable output paths
- Add batch processing for large codebases
- Include diff context configuration options
- Add support for custom review criteria/prompts

## Example Output Structure

The generated review file follows this format:
```markdown
# ðŸ§  Claude Review

_Last updated: 2024-01-15 14:30_

## Summary
[Claude's overall assessment]

## Issues Found
- [Specific bugs or problems]

## Suggestions
- [Improvement recommendations]

## Code Quality Notes
[Style and clarity feedback]
```